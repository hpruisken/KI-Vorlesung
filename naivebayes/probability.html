<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="height=device-height, width=device-width, initial-scale=1.0, minimum-scale=1.0">
    <meta name="generator" content="Hugo 0.102.1">
    <meta name="generator" content="Relearn 5.2.3+tip">
    <meta name="description" content="">
    <meta name="author" content="cagix">
    <title>Wiederholung Wahrscheinlichkeitstheorie   </title>

    
    

    <link href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/images/logo.png?1667412025" rel="icon" type="image/png">

    <!-- https://github.com/filamentgroup/loadCSS/blob/master/README.md#how-to-use -->
    <link href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/css/fontawesome-all.min.css?1667412025" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/css/fontawesome-all.min.css?1667412025" rel="stylesheet"></noscript>
    <link href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/css/featherlight.min.css?1667412025" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/css/featherlight.min.css?1667412025" rel="stylesheet"></noscript>
    <link href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/css/auto-complete.css?1667412025" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/css/auto-complete.css?1667412025" rel="stylesheet"></noscript>
    <link href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/css/perfect-scrollbar.min.css?1667412025" rel="stylesheet">
    <link href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/css/nucleus.css?1667412025" rel="stylesheet">
    <link href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/css/fonts.css?1667412025" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/css/fonts.css?1667412025" rel="stylesheet"></noscript>
    <link href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/css/theme.css?1667412025" rel="stylesheet">
    <link href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/css/theme-relearn-light.css?1667412025" rel="stylesheet" id="variant-style">
    <link href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/css/ie.css?1667412025" rel="stylesheet">
    <link href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/css/variant.css?1667412025" rel="stylesheet">
    <link href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/css/print.css?1667412025" rel="stylesheet" media="print">
    <script src="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/js/variant.js?1667412025"></script>
    <script>
      // hack to let hugo tell us how to get to the root when using relativeURLs, it needs to be called *url= for it to do its magic:
      // https://github.com/gohugoio/hugo/blob/145b3fcce35fbac25c7033c91c1b7ae6d1179da8/transform/urlreplacers/absurlreplacer.go#L72
      var index_url="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/index.json";
      var root_url="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/";
      var baseUri=root_url.replace(/\/$/, '');
      // translations
      window.T_Copy_to_clipboard = 'Copy to clipboard';
      window.T_Copied_to_clipboard = 'Copied to clipboard!';
      window.T_Copy_link_to_clipboard = 'Copy link to clipboard';
      window.T_Link_copied_to_clipboard = 'Copied link to clipboard!';
      // some further base stuff
      var baseUriFull='https:\/\/www.fh-bielefeld.de\/elearning\/data\/FH-Bielefeld\/lm_data\/lm_1358898/';
      window.variants && variants.init( [ 'relearn-light', 'relearn-dark', 'green' ] );
    </script>
    <script src="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/js/jquery.min.js?1667412025" defer></script><style type="text/css">

 
.center {
    align-content: center;
    text-align: center;
    margin: auto;
}
.alert {
    color: #ff3333;
}
.bsp {
    padding: 0.05cm;
    border-width: 0.05cm;
    border-style: solid;
    border-color: #ddd;
    background-color: #ddd;
    border-radius: 25px;
    float: right;
}
.cbox {
    padding: 0.2cm;
    border-width: 0.1cm;
    border-style: solid;
    border-color: #4070a0;
    background-color: #f2f2f2;
    margin: auto;
    width: 60%;
    text-align: center;
    overflow: auto;
}
.blueArrow {
    color: #4070a0;
    font-family: "Courier New", "Courier", monospace;
    font-weight: bold;
}
.origin {
    background-color: #ededed;
    font-size: 0.8em;
}
.showme {
    background-color: #ededed;
    font-size: 0.8em;
}


 
.tldr {
    background: #dbe4ed;
    padding: 12px;
    margin: 4px 0px 26px 0px;
}
.recap {
    
    
   margin: 4px 0px 26px 0px;
}
.bib {
    background: #dbe4ed;
    padding: 12px;
    margin: 4px 0px 26px 0px;
}
.outcomes {
    background: #d9e9d5;
    padding: 12px;
    margin: 4px 0px 26px 0px;
}
.quizzes {
    background: #d9e9d5;
    padding: 12px;
    margin: 4px 0px 26px 0px;
}
.challenges {
    background: #ebe4d6;
    padding: 12px;
    margin: 4px 0px 26px 0px;
}
.assignments {
    background: #ebe4d6;
    padding: 12px;
    margin: 4px 0px 26px 0px;
}
h1.tldr, h1.recap, h1.bib, h1.outcomes, h1.quizzes, h1.challenges, h1.assignments {
    padding: 0px;
}


 
.noJsAlert {
    padding: 20px;
    background-color: #f44336;  
    color: white;
    margin-bottom: 15px;
}


 
.embed-video-player {
    position: relative;
    padding-bottom: 56%;
    height: 0;
    overflow: hidden;
}
.youtube-player {
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    border:0;
}


 
#header-wrapper {
    padding:0.6rem;
}


 
#shortcuts {
    padding-top: 2.0rem;
}


 
#chapter p {
    text-align: left;
}


 
figcaption h4 {
    margin-top:-2.5rem;
}
.border1 {
    border:1px solid black;
}

 
td ul, td ol {
    margin: 0 0 1rem 0.5rem;
    padding: 0 0 0 0.5rem;
}

 
h1 { font-size:2.8rem !important;}
h2 { font-size:2.2rem; margin:1.2rem 0}
h3 { font-size:1.9rem; text-align:left !important; font-weight:400 !important;}
h4 { font-size:1.6rem}
h5 { font-size:1.3rem}
h6 { font-size:1rem}

h2 {
    width:100% !important;
    border-bottom:1px solid #5e5e5e !important;
    padding-bottom: 2px;
}
.tldr h2, .recap h2, .bib h2, .outcomes h2, .quizzes h2, .challenges h2, .assignments h2 {
    margin:0.5rem 0
}

.btn-crossreference, .btn-crossreference:hover {
    cursor: initial;
}

</style>

  </head>
  <body class="mobile-support default disableInlineCopyToClipboard" data-url="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/naivebayes/probability.html">
    <div id="body" class="default-animation">
      <div id="sidebar-overlay"></div>
      <div id="toc-overlay"></div>
      <nav id="topbar" class="highlightable">
        <div>
          <div id="top-github-link">
            <a class="github-link" title='Edit (CTRL+ALT+e)' href="https://github.com/Artificial-Intelligence-FHB-TDU/KI-Vorlesung/edit/master/markdown/naivebayes/probability.md" target="blank">
              <i class="fas fa-pen fa-fw"></i>
            </a>
          </div>
          <div id="breadcrumbs">
            <span id="sidebar-toggle-span">
              <a href="#" id="sidebar-toggle" title='Menu (CTRL+ALT+m)'><i class="fas fa-bars fa-fw"></i></a>
            </span>
            <span id="toc-menu" title='Table of Contents (CTRL+ALT+t)'><i class="fas fa-list-alt fa-fw"></i></span>
            <span class="links">
              Wiederholung Wahrscheinlichkeitstheorie
            </span>
          </div>
          <div class="default-animation progress">
            <div class="wrapper">
<nav id="TableOfContents">
  <ul>
    <li><a href="#ereignisse-und-wahrscheinlichkeit">Ereignisse und Wahrscheinlichkeit</a>
      <ul>
        <li><a href="#ereignisse">Ereignisse</a></li>
        <li><a href="#regeln">Regeln</a></li>
        <li><a href="#wahrscheinlichkeit">Wahrscheinlichkeit</a></li>
        <li><a href="#verteilung">Verteilung</a></li>
        <li><a href="#beispiel">Beispiel</a></li>
      </ul>
    </li>
    <li><a href="#rechenregeln-kolmogorov-axiome">Rechenregeln: Kolmogorov Axiome</a></li>
    <li><a href="#verbundwahrscheinlichkeiten">Verbundwahrscheinlichkeiten</a></li>
    <li><a href="#bedingte-wahrscheinlichkeit">Bedingte Wahrscheinlichkeit</a></li>
    <li><a href="#marginalisierung">Marginalisierung</a></li>
    <li><a href="#kettenregel">Kettenregel</a></li>
    <li><a href="#bayes-regel">Bayes-Regel</a></li>
    <li><a href="#beispiel-bayes">Beispiel Bayes</a></li>
    <li><a href="#unabhängige-ereignisse">Unabhängige Ereignisse</a></li>
    <li><a href="#wrap-up">Wrap-Up</a></li>
  </ul>
</nav>
            </div>
          </div>
        </div>
      </nav>
      <main id="body-inner" class="highlightable " tabindex="-1">
        <div class="flex-block-wrapper">
          <div id="head-tags">
          </div>
<article class="default">
<h1>Wiederholung Wahrscheinlichkeitstheorie</h1>



    





<div class="tldr">
    <h2>TL;DR</h2>

    
        <p>Diese Sitzung ist eine (relativ oberflächliche) Einführung/Wiederholung in die/der
Grundlagen der Wahrscheinlichkeitstheorie.</p>
<p>Wir schauen uns die möglichen Ausgänge eines Zufallsexperiments an (&quot;Ereignisse&quot;).
Wenn diese Ereignisse sich gegenseitig ausschließen und alle denkbaren Ergebnisse
abdecken, dann nennt man diese Ereignisse auch <strong>Elementarereignisse</strong>. Die
Wahrscheinlichkeit für ein Ereignis kann man angeben als Anzahl der möglichen
Ergebnisse, die für dieses Ereignis günstig sind, geteilt durch die Anzahl aller
Ausgänge. Über die Kolmogorov Axiome bekommt man die typischen Rechenregel für
die Wahrscheinlichkeit.</p>
<p>Man kann eine <strong>Verbundwahrscheinlichkeit</strong> 
<span class="math align-center">$P(A,B) = P(B,A)$</span> angeben, das ist
die Wahrscheinlichkeit, dass 
<span class="math align-center">$A$</span> und 
<span class="math align-center">$B$</span> gleichzeitig auftreten.</p>
<p>Die <strong>bedingte</strong> Wahrscheinlichkeit für 
<span class="math align-center">$A$</span> gegeben 
<span class="math align-center">$B$</span> ist 
<span class="math align-center">$P(A|B)$</span> und berechnet
sich 
<span class="math align-center">$P(A|B) = \frac{P(A,B)}{P(B)}$</span>.</p>
<p>Daraus kann man die <strong>Bayes-Regel</strong> ableiten: 
<span class="math align-center">$$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$</span>
Dabei nennt man</p>
<ul>
<li>
<span class="math align-center">$P(A)$</span> <strong>&quot;Prior&quot;</strong> oder <strong>&quot;A-priori-Wahrscheinlichkeit&quot;</strong>
(die Wahrscheinlichkeit für 
<span class="math align-center">$A$</span> ohne weiteres Wissen),</li>
<li>
<span class="math align-center">$P(B|A)$</span> <strong>&quot;Likelihood&quot;</strong>
(Wie wahrscheinlich ist das Auftreten von 
<span class="math align-center">$B$</span>, gegeben 
<span class="math align-center">$A$</span>?),</li>
<li>
<span class="math align-center">$P(A|B)$</span> <strong>&quot;Posterior&quot;</strong> oder <strong>&quot;A-posteriori-Wahrscheinlichkeit&quot;</strong>
(Wie wahrscheinlich ist 
<span class="math align-center">$A$</span>, wenn 
<span class="math align-center">$B$</span> eingetreten ist?), und</li>
<li>
<span class="math align-center">$P(B)$</span> ist ein Normierungsfaktor
(Wie wahrscheinlich ist 
<span class="math align-center">$B$</span> an sich?).</li>
</ul>

    

    
        <h3>Videos (YouTube)</h3>
        
        
        


<ul>

    

    
        <li><a href="https://youtu.be/p_Yy5rkl4CA" target="_blank" rel="nofollow noopener noreferrer">VL Wahrscheinlichkeiten</a></li>
    

</ul>

    

    
        <h3>Videos (FH-Medienportal)</h3>
        
        
        


<ul>

    

    
        <li><a href="https://www.fh-bielefeld.de/medienportal/m/91611d22ad88ef1adf04c3956746256d14c609274091660652ff567645a537a8983f61cf6c3406110e27dc6cb56b65b599d72d60ab37f45977b6732f6610830d" target="_blank" rel="nofollow noopener noreferrer">VL Wahrscheinlichkeiten</a></li>
    

</ul>

    

    

</div>


    



<div class="outcomes">
    <h2>Lernziele</h2>

    <ul>
    
        
        <li>(K2) Elementarereignisse und Wahrscheinlichkeit</li>
        
        
    
        
        <li>(K2) Bedingte Wahrscheinlichkeit und Verbundwahrscheinlichkeit</li>
        
        
    
        
        <li>(K2) (Bedingte) Unabhängigkeit</li>
        
        
    
        
        
        <li>(K3) Rechenregeln</li>
        
    
        
        
        <li>(K3) Marginalisierung</li>
        
    
        
        
        <li>(K3) Bayes'sche Regel</li>
        
    
    </ul>

</div>


    <h2 id="ereignisse-und-wahrscheinlichkeit">Ereignisse und Wahrscheinlichkeit</h2>
<p><strong>Hinweis</strong>: Die folgende Darstellung zur Einführung in die
Wahrscheinlichkeitstheorie dient dem Verständnis des Naive Bayes
Klassifikationsalgorithmus und ist teilweise eher oberflächlich gehalten.
Sie kann und soll keine entsprechende mathematische Einführung ersetzen!</p>
<h3 id="ereignisse">Ereignisse</h3>
<ul>
<li>
<p><strong>Ereignisse</strong> 
<span class="math align-center">$\Omega = \lbrace \omega_1, \omega_2, \ldots, \omega_n \rbrace$</span>:
endliche Menge der Ausgänge eines Zufallsexperiments</p>
</li>
<li>
<p><strong>Elementarereignis</strong>: Die 
<span class="math align-center">$\omega_i \in \Omega$</span></p>
<ul>
<li>decken <em>alle</em> möglichen Versuchsergebnisse ab, und</li>
<li>schließen sich gegenseitig aus</li>
</ul>
</li>
</ul>
<h3 id="regeln">Regeln</h3>
<ul>
<li>Wenn 
<span class="math align-center">$A$</span> und 
<span class="math align-center">$B$</span> Ereignisse sind, dann auch 
<span class="math align-center">$A \cup B$</span></li>
<li>
<span class="math align-center">$\Omega$</span> wird als <strong>sicheres Ereignis</strong> bezeichnet: Enthält
definitionsgemäß <strong>alle</strong> Versuchsausgänge, d.h. <em>ein</em> in der Menge
enthaltenes Ereignis <em>muss</em> auftreten</li>
<li>Die leere Menge 
<span class="math align-center">$\emptyset$</span> wird als <strong>unmögliches Ereignis</strong> bezeichnet</li>
<li>Die Variablen 
<span class="math align-center">$A$</span> und 
<span class="math align-center">$B$</span> heißen auch <strong>Zufallsvariablen</strong></li>
</ul>
<p>Im Rahmen dieser Veranstaltung betrachten wir nur diskrete Zufallsvariablen mit
endlichem Wertebereich!</p>
<h3 id="wahrscheinlichkeit">Wahrscheinlichkeit</h3>
<ul>
<li>
<p><strong>Wahrscheinlichkeit</strong>:</p>
<p>Sei 
<span class="math align-center">$\Omega = \lbrace \omega_1, \omega_2, \ldots, \omega_n \rbrace$</span> endlich.
Die Wahrscheinlichkeit 
<span class="math align-center">$P(A)$</span> für ein Ereignis 
<span class="math align-center">$A$</span> ist dann
definiert als</p>

<span class="math align-center">$$
    P(A) = \frac{|A|}{|\Omega|} =
    \frac{\text{Anzahl der für A günstigen Fälle}}{\text{Anzahl der möglichen Fälle}}
    $$</span>
<p>Man könnte auch schreiben: 
<span class="math align-center">$P(A) = \sum_{\omega \in A} P(\omega)$</span></p>
<p><em>Hinweis</em>: Diese Definition von Wahrscheinlichkeit geht von gleich
wahrscheinlichen Elementarereignissen aus! Die allgemeine Definition
geht über einen entsprechenden Grenzwert.</p>
</li>
</ul>
<h3 id="verteilung">Verteilung</h3>
<p>Den Vektor mit den Wahrscheinlichkeiten aller Elementarereignisse
nennt man auch <em>Verteilung</em>.</p>
<p>Beispiel: 
<span class="math align-center">$\mathbf{P}(A) = (P(A=1), P(A=2), \ldots, P(A=6)) = (1/6, 1/6, \ldots, 1/6)$</span></p>
<p><em>Hinweis</em>: Wir betrachten hier nur diskrete Zufallsvariablen. Für
kontinuierliche Variablen wird die Verteilung mit Hilfe einer
<strong>Dichtefunktion</strong> dargestellt, beispielsweise der Gauss'schen Funktion.</p>
<h3 id="beispiel">Beispiel</h3>
<ul>
<li>Einmaliges Würfeln mit einem Spielwürfel: 
<span class="math align-center">$\Omega = \lbrace 1,2,3,4,5,6 \rbrace$</span></li>
<li>Elementarereignisse: 
<span class="math align-center">$\lbrace 1,2,3,4,5,6 \rbrace$</span></li>
<li>Das Würfeln einer geraden Zahl (
<span class="math align-center">$A = \lbrace 2,4,6 \rbrace$</span>) ist <em>kein</em>
Elementarereignis, ebenso wie das Würfeln einer Zahl kleiner 5
(
<span class="math align-center">$B = \lbrace 1,2,3,4 \rbrace$</span>), da 
<span class="math align-center">$A \cap B = \lbrace 2,4 \rbrace \ne \emptyset$</span></li>
<li>Wahrscheinlichkeit, eine 1 zu würfeln: 
<span class="math align-center">$P(A \in \lbrace 1 \rbrace) = P(A=1) = \frac{1}{6}$</span>.
<em>Anmerkung</em>: Man schreibt statt 
<span class="math align-center">$P(A \in \lbrace 1 \rbrace)$</span> oft einfach 
<span class="math align-center">$P(1)$</span>.</li>
<li>Wahrscheinlichkeit, eine gerade Zahl zu würfeln:

<span class="math align-center">$P(A \in \lbrace 2,4,6 \rbrace) = P(A=2 \vee A=4 \vee A=6) = \frac{|\lbrace 2,4,6 \rbrace|}{|\lbrace 1,2,3,4,5,6 \rbrace|} = \frac{3}{6} = 0.5$</span></li>
</ul>
<h2 id="rechenregeln-kolmogorov-axiome">Rechenregeln: Kolmogorov Axiome</h2>
<p>Sei 
<span class="math align-center">$A$</span> ein Ereignis, also 
<span class="math align-center">$A \subseteq \Omega$</span>:</p>
<ul>
<li>

<span class="math align-center">$0 \le P(A) \le 1$</span>
</li>
<li>
<p>
<span class="math align-center">$\Omega = \lbrace \omega_1, \omega_2, \ldots, \omega_n \rbrace$</span>: 
<span class="math align-center">$\sum_{i} P(\omega_i) = 1$</span>
(Normierungsbedingung: Summe über die Wahrscheinlichkeiten aller Elementarereignisse ist immer 1)</p>
</li>
<li>

<span class="math align-center">$P(A \cup B) = P(A) + P(B) - P(A \cap B)$</span>
</li>
</ul>
<p>Daraus folgt (u.a.):</p>
<ul>
<li>

<span class="math align-center">$P(\Omega) = 1$</span>
</li>
<li>

<span class="math align-center">$P(\emptyset) = 0$</span>
</li>
<li>

<span class="math align-center">$P(A) = 1- P(\neg A)$</span>
</li>
<li>
<p>
<span class="math align-center">$A$</span> und 
<span class="math align-center">$B$</span> <em>unabhängig</em>: 
<span class="math align-center">$P(A \cup B) = P(A) + P(B)$</span></p>
</li>
<li>
<p>
<span class="math align-center">$P(A \cap B)$</span> ist leer, wenn 
<span class="math align-center">$A$</span> und 
<span class="math align-center">$B$</span> sich nicht überlappen</p>
</li>
<li>
<p>
<span class="math align-center">$A \subseteq B$</span>: 
<span class="math align-center">$P(A) \le P(B)$</span></p>
</li>
</ul>
<div style="text-align: right;">
<span class="btn cstyle btn-crossreference">
  <a>
    Tafel: Würfeln: Elementar, gerade Zahl
  </a>
</span></div>
<h2 id="verbundwahrscheinlichkeiten">Verbundwahrscheinlichkeiten</h2>

<span class="math align-center">$$P(A,B) = P(B,A) = \text{ Wahrscheinlichkeit, dass A und B gleichzeitig auftreten }$$</span>
<table>
<thead>
<tr>
<th></th>
<th>Halsschmerzen</th>
<th>
<span class="math align-center">$\neg$</span> Halsschmerzen</th>
</tr>
</thead>
<tbody>
<tr>
<td>Schnupfen</td>
<td>0.04</td>
<td>0.06</td>
</tr>
<tr>
<td>
<span class="math align-center">$\neg$</span> Schnupfen</td>
<td>0.01</td>
<td>0.89</td>
</tr>
</tbody>
</table>
<ul>
<li>
<span class="math align-center">$P(S,H) = 0.04$</span></li>
</ul>
<p>Die Tabelle kann man so lesen: In 4 von 100 Fällen tritt das Ereignis &quot;Schnupfen&quot;
gleichzeitig mit dem Ereignis &quot;Halsschmerzen&quot; auf, in 6 von 100 Fällen tritt
&quot;Schupfen&quot; ohne Halsschmerzen auf. ... In Summe kommt man wieder auf 100 Fälle
(100 Prozent).</p>
<p>Nach diesen Zahlen liegt also die Verbundwahrscheinlichkeit für die Ereignisse
&quot;Schnupfen&quot; und &quot;Husten&quot;, d.h. 
<span class="math align-center">$P(S,H)$</span>, bei 4 Prozent.</p>
<p><strong>Hinweis</strong>: Die gezeigten Zahlen und Zusammenhänge sind <strong>fiktiv</strong>
und dienen lediglich zur Verdeutlichung der Wahrscheinlichkeitsbegriffe!</p>
<h2 id="bedingte-wahrscheinlichkeit">Bedingte Wahrscheinlichkeit</h2>
<p><strong>Definition</strong>:
Bedingte Wahrscheinlichkeit für 
<span class="math align-center">$A$</span> gegeben 
<span class="math align-center">$B$</span>:</p>

<span class="math align-center">$$P(A|B) = \frac{P(A,B)}{P(B)}$$</span>
<table>
<thead>
<tr>
<th></th>
<th>Halsschmerzen</th>
<th>
<span class="math align-center">$\neg$</span> Halsschmerzen</th>
</tr>
</thead>
<tbody>
<tr>
<td>Schnupfen</td>
<td>0.04</td>
<td>0.06</td>
</tr>
<tr>
<td>
<span class="math align-center">$\neg$</span> Schnupfen</td>
<td>0.01</td>
<td>0.89</td>
</tr>
</tbody>
</table>
<ul>
<li>
<span class="math align-center">$P(\text{Schnupfen } | \text{ Halsschmerzen}) = \frac{P(S,H)}{P(H)} = \frac{0.04}{0.04+0.01} = 0.8$</span></li>
<li>
<span class="math align-center">$P(\text{Halsschmerzen } | \text{ Schnupfen}) = \frac{P(H,S)}{P(S)} = \frac{0.04}{0.04+0.06} = 0.4$</span></li>
</ul>
<p>Wegen 
<span class="math align-center">$P(A|B) = \dfrac{P(A,B)}{P(B)}$</span> ist 
<span class="math align-center">$P(A,B) = P(A|B)P(B) = P(B|A)P(A)$</span>
(<strong>Produkt-Regel</strong>)!</p>
<h2 id="marginalisierung">Marginalisierung</h2>
<table>
<thead>
<tr>
<th></th>
<th>Halsschmerzen</th>
<th>
<span class="math align-center">$\neg$</span> Halsschmerzen</th>
<th>
<span class="math align-center">$\sum$</span></th>
</tr>
</thead>
<tbody>
<tr>
<td>Schnupfen</td>
<td>0.04</td>
<td>0.06</td>
<td><em>0.1</em></td>
</tr>
<tr>
<td>
<span class="math align-center">$\neg$</span> Schnupfen</td>
<td>0.01</td>
<td>0.89</td>
<td><em>0.9</em></td>
</tr>
<tr>
<td>
<span class="math align-center">$\sum$</span></td>
<td><em>0.05</em></td>
<td><em>0.95</em></td>
<td><em>1</em></td>
</tr>
</tbody>
</table>

<span class="math align-center">$P(S) = P(S,H) + P(S, \neg H)$</span>
<p>Allgemein:
Seien 
<span class="math align-center">$B_1, \ldots, B_n$</span> Elementarereignisse mit 
<span class="math align-center">$\bigcup_i B_i = \Omega$</span>.
Dann ist 
<span class="math align-center">$$P(A) = \sum_i P(A,B_i) = \sum_i P(A|B_i)P(B_i)$$</span></p>
<p>Diesen Vorgang nennt man <strong>Marginalisierung</strong>. Die resultierende Verteilung

<span class="math align-center">$P(A)$</span> nennt man auch <em>&quot;Randverteilung&quot;</em>, da sie mit einer Projektion eines
Quaders auf eine Seitenfläche vergleichbar ist.</p>
<h2 id="kettenregel">Kettenregel</h2>
<ul>
<li>
<p><strong>Produktregel</strong>: Wegen 
<span class="math align-center">$P(A|B) = \dfrac{P(A,B)}{P(B)}$</span>
gilt 
<span class="math align-center">$P(A,B) = P(A|B)P(B)$</span></p>
</li>
<li>
<p>Verallgemeinerung (<strong>Kettenregel</strong>):

<span class="math align-center">$$
    \begin{array}{rcl}
    P(A_1,A_2,\ldots,A_n) &=& P(A_n,\ldots,A_2,A_1)\\
        & = & P(A_n|A_{n-1},\ldots,A_1)P(A_{n-1},\ldots,A_1)\\
        & = & P(A_n|A_{n-1},\ldots,A_1)P(A_{n-1}|A_{n-2},\ldots,A_1)P(A_{n-2},\ldots,A_1)\\
        & = & \ldots\\
        & = & P(A_n|A_{n-1},\ldots,A_1) \ldots P(A_2|A_1)P(A_1)\\
        & = & \prod_i P(A_i|A_1,\ldots,A_{i-1})
    \end{array}
    $$</span></p>
</li>
</ul>
<h2 id="bayes-regel">Bayes-Regel</h2>
<p>Bedingte Wahrscheinlichkeit: 
<span class="math align-center">$P(A,B) = P(A|B)P(B) = P(B|A)P(A)$</span></p>

<span class="math align-center">$$
P(A|B) = \frac{P(B|A)P(A)}{P(B)}
$$</span>
<ul>
<li>
<span class="math align-center">$P(A)$</span> nennt man <strong>&quot;Prior&quot;</strong> oder <strong>&quot;A-priori-Wahrscheinlichkeit&quot;</strong>
(Das ist die Wahrscheinlichkeit für 
<span class="math align-center">$A$</span> ohne weiteres Wissen)</li>
<li>
<span class="math align-center">$P(B|A)$</span> nennt man <strong>&quot;Likelihood&quot;</strong>
(Wie wahrscheinlich ist das Auftreten von 
<span class="math align-center">$B$</span>, gegeben 
<span class="math align-center">$A$</span>?)</li>
<li>
<span class="math align-center">$P(A|B)$</span> nennt man <strong>&quot;Posterior&quot;</strong> oder <strong>&quot;A-posteriori-Wahrscheinlichkeit&quot;</strong>
(Wie wahrscheinlich ist 
<span class="math align-center">$A$</span>, wenn 
<span class="math align-center">$B$</span> eingetreten ist?)</li>
<li>
<span class="math align-center">$P(B)$</span> ist ein Normierungsfaktor</li>
</ul>
<p>Wenn man (siehe später: Naive Bayes Klassifikator) 
<span class="math align-center">$A$</span> als Klasse und 
<span class="math align-center">$B$</span> als
Daten betrachtet:</p>
<ul>
<li>
<span class="math align-center">$P(A)$</span>: Wie wahrscheinlich ist eine bestimmte Klasse an sich
(A-priori-Wahrscheinlichkeit der Klassen)?</li>
<li>
<span class="math align-center">$P(B|A)$</span>: Wie wahrscheinlich sind bestimmte Daten, gegeben die Klasse 
<span class="math align-center">$A$</span>?
(Likelihood der Daten)</li>
<li>
<span class="math align-center">$P(A|B)$</span>: Gegeben die Daten 
<span class="math align-center">$B$</span>, wie wahrscheinlich ist die Klasse 
<span class="math align-center">$A$</span>?
(Posterior)</li>
</ul>
<p>In der Medizin hat sucht man i.d.R. die Ursache für beobachtete Symptome:

<span class="math align-center">$$
P(\text{Ursache}|\text{Symptome}) = \frac{P(\text{Symptome}|\text{Ursache})P(\text{Ursache})}{P(\text{Symptome})}
$$</span></p>
<p>Aus der A-priori-Wahrscheinlichkeit für bestimmte Krankheiten und der
Likelihood der Symptome (wie wahrscheinlich sind Symptome, gegeben eine
Krankheit) kann man die Wahrscheinlichkeit für das Vorliegen einer Erkrankung
gegeben bestimmte Symptome berechnen.</p>
<h2 id="beispiel-bayes">Beispiel Bayes</h2>
<ul>
<li>Bei Arthrose wird in 80 Prozent der Fälle ein steifes Gelenk beobachtet</li>
<li>Eine von 10.000 Personen hat Arthrose</li>
<li>Eine von 10 Personen hat ein steifes Gelenk</li>
</ul>
<p>=&gt; Ich habe ein steifes Gelenk. Habe ich Arthrose?</p>
<ul>
<li>Gegeben: 
<span class="math align-center">$P(A) = 0.0001, P(S) = 0.1, P(S|A) = 0.8$</span></li>
<li>Gesucht: 
<span class="math align-center">$P(A|S)$</span></li>
</ul>

<span class="math align-center">$$
P(A|S) = \frac{P(S|A)P(A)}{P(S)} = \frac{0.8 \times 0.0001}{0.1} = 0.0008 = 0.08\%
$$</span>
<p>Wenn ein steifes Gelenk vorliegt, ist die Wahrscheinlichkeit, dann an Arthrose
erkrankt zu sein, bei nur 0.08%. Kein Grund zur Sorge in diesem Fall :-)</p>
<p>=&gt; Wie wahrscheinlich ist ein steifes Gelenk ohne Arthrose, also 
<span class="math align-center">$P(S|\neg A$</span>)?</p>
<p>Mit Marginalisierung: 
<span class="math align-center">$P(S) = P(S|A)P(A) + P(S|\neg A)P(\neg A)$</span>,
d.h. 
<span class="math align-center">$0.1 = 0.8 \times 0.0001 + P(S|\neg A) \times (1-0.0001)$</span>,
d.h. 
<span class="math align-center">$P(S|\neg A) = 0.0999$</span></p>
<p>In knapp 10 Prozent der Fälle würde man im obigen Beispiel bei der Diagnose
&quot;keine Arthrose&quot; ein steifes Gelenk beobachten.</p>
<p><strong>Hinweis</strong>: Die genannten Zahlen und Zusammenhänge sind rein fiktional und sollen
lediglich zur Veranschaulichung der Bayes-Regel dienen!</p>
<p>Schauen Sie sich auch das Beispiel 7.9 in <a href="#id_Ertel2017">[Ertel2017, Ex. 7.9, S. 135]</a> an!</p>
<h2 id="unabhängige-ereignisse">Unabhängige Ereignisse</h2>
<ul>
<li>

<span class="math align-center">$P(\text{Halsschmerzen},\text{ Regen}) = P(\text{Regen }|\text{ Halsschmerzen})P(\text{Halsschmerzen})$</span>
</li>
<li>
<p>
<span class="math align-center">$P(\text{Regen }|\text{ Halsschmerzen}) = \text{ ?? }$</span> 
<span class="math align-center">$= P(\text{Regen})$</span></p>
</li>
<li>
<p>Zwei Ereignisse 
<span class="math align-center">$A$</span> und 
<span class="math align-center">$B$</span> sind <strong>unabhängig</strong>, wenn

<span class="math align-center">$$ P(A|B) = P(A) $$</span></p>
<p>=&gt; 
<span class="math align-center">$P(A,B) = P(A|B)P(B) = P(A)P(B)$</span></p>
</li>
</ul>
<p>Dies kann man verallgemeinern (<strong>bedingte Unabhängigkeit</strong>):</p>
<blockquote>
<p>
<span class="math align-center">$X$</span> und 
<span class="math align-center">$Y$</span> sind <em>bedingt unabhängig</em> (gegeben 
<span class="math align-center">$Z$</span>),
wenn 
<span class="math align-center">$P(X|Y,Z) = P(X|Z)$</span> bzw. 
<span class="math align-center">$P(Y|X,Z) = P(Y|Z)$</span></p>
</blockquote>
<p>Daraus folgt:</p>

<span class="math align-center">$$ P(X,Y|Z) = P(X|Y,Z)P(Y|Z) = P(X|Z)P(Y|Z) $$</span>
<h2 id="wrap-up">Wrap-Up</h2>
<ul>
<li>Grundlagen der Wahrscheinlichkeitstheorie
<ul>
<li>Elementarereignisse und Wahrscheinlichkeit</li>
<li>Rechenregeln</li>
<li>Bedingte Wahrscheinlichkeit und Verbundwahrscheinlichkeit</li>
<li>Marginalisierung</li>
<li>(Bedingte) Unabhängigkeit</li>
<li>Bayes'sche Regel</li>
</ul>
</li>
</ul>

    


<div class="quizzes">
    <h2>Quizzes</h2>

    <ul>
    
        
        
        
        <li>
            <a href="https://www.fh-bielefeld.de/elearning/goto.php?target=tst_1106587&amp;client_id=FH-Bielefeld" class="icon reading" target="_blank" rel="nofollow noopener noreferrer"><strong>Selbsttest Wahrscheinlichkeiten (ILIAS)</strong></a>
        </li>
        
    
    </ul>

</div>


    






    



<div class="bib">
    <h2>Quellen</h2>

    <ul>
    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
            
            
            
            
            

            <li id="id_Ertel2017">
            
                [Ertel2017] <strong>Introduction to Artificial Intelligence</strong>
            
            
                <br>Ertel, W., Springer, 2017.
                
                    ISBN <a href="https://www.digibib.net/openurl/Bi10?isbn=978-3-319-58487-4" target="_blank" rel="nofollow noopener noreferrer">978-3-319-58487-4</a>.
                
                
                    DOI <a href="https://doi.org/10.1007/978-3-319-58487-4" target="_blank" rel="nofollow noopener noreferrer">10.1007/978-3-319-58487-4</a>.
                
            
            
                <br><em>Abschnitt 7.1</em>
            
            </li>
        

    
        
        

        
            
            
            
            
            

            
            
                
                    
                
            
                
                    
                
            
            
            
            
            
            

            <li id="id_Russell2020">
            
                [Russell2020] <a href="http://aima.cs.berkeley.edu" class="icon reading" target="_blank" rel="nofollow noopener noreferrer"><strong>Artificial Intelligence: A Modern Approach</strong></a>
            
            
                <br>Russell, S. und Norvig, P., Pearson, 2020.
                
                    ISBN <a href="https://www.digibib.net/openurl/Bi10?isbn=978-0134610993" target="_blank" rel="nofollow noopener noreferrer">978-0134610993</a>.
                
                
            
            
                <br><em>Kapitel 12</em>
            
            </li>
        

    
    </ul>

</div>





<footer class="footline"><div style="color: darkgray; font-size: small;">
<p style="margin: 4rem;">
<!-- https://creativecommons.org/choose/ -->
<a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
Unless otherwise noted, <a href="https://github.com/Artificial-Intelligence-FHB-TDU/KI-Vorlesung">this work</a> by <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cyildiz" property="cc:attributionName" rel="cc:attributionURL">Canan Yıldız</a>, <a xmlns:cc="https://creativecommons.org/ns#" href="https://github.com/cagix" property="cc:attributionName" rel="cc:attributionURL">Carsten Gips</a>, and <a href="https://github.com/Artificial-Intelligence-FHB-TDU/KI-Vorlesung/graphs/contributors">contributors</a> is licensed under <a rel="license" href="https://github.com/Artificial-Intelligence-FHB-TDU/KI-Vorlesung/blob/master/LICENSE.md">CC BY-SA 4.0</a>.
See the <a href="https://github.com/Artificial-Intelligence-FHB-TDU/KI-Vorlesung/blob/master/CREDITS.md">credits</a> for a detailed list of contributing projects.

</p>
</div>

</footer>
</article>
        </div>
      </main>
    </div>
    <aside id="sidebar" class="default-animation showVisitedLinks">
      <div id="header-wrapper" class="default-animation">
        <div id="header" class="default-animation">




<a id="logo" href='https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898//index.html'>
    <img style="max-width:35%" alt="icon" src='https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/images/logo.png'>
</a>

        </div>
      </div>
      <div id="content-wrapper" class="highlightable">
        <ul class="topics">
          <li data-nav-id="/intro.html" title="Einführung KI" class="dd-item"><input type="checkbox" id="section-925e5bfbd01266c727d74ae1db5ab0c6" class="toggle"/><label for="section-925e5bfbd01266c727d74ae1db5ab0c6" ></label><a href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/intro.html">Einführung KI<i class="fas fa-check read-icon"></i></a><ul>
          <li data-nav-id="/intro/intro-ai.html" title="Intro: Was ist Künstliche Intelligenz?" class="dd-item"><a href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/intro/intro-ai.html">Einführung KI<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/intro/problems.html" title="Problemlösen" class="dd-item"><a href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/intro/problems.html">Problemlösen<i class="fas fa-check read-icon"></i></a></li></ul></li>
          <li data-nav-id="/search.html" title="Suche" class="dd-item"><input type="checkbox" id="section-08cfb6301b533c954681b7845aa7e62a" class="toggle"/><label for="section-08cfb6301b533c954681b7845aa7e62a" ></label><a href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/search.html">Suche<i class="fas fa-check read-icon"></i></a><ul>
          <li data-nav-id="/search/uninformed.html" title="Uninformierte Suche" class="dd-item alwaysopen"><input type="checkbox" id="section-81ce67a4c4f848b49be6e8b0a77d0660" class="toggle" checked/><label for="section-81ce67a4c4f848b49be6e8b0a77d0660" ></label><a href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/search/uninformed.html">Uninformierte Suche<i class="fas fa-check read-icon"></i></a><ul>
          <li data-nav-id="/search/uninformed/dfs.html" title="Suche mit Tiefensuche" class="dd-item"><a href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/search/uninformed/dfs.html">Tiefensuche<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/search/uninformed/bfs.html" title="Suche mit Breitensuche" class="dd-item"><a href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/search/uninformed/bfs.html">Breitensuche<i class="fas fa-check read-icon"></i></a></li></ul></li>
          <li data-nav-id="/search/informed.html" title="Informierte Suche" class="dd-item alwaysopen"><input type="checkbox" id="section-e57a324c0a87423ed05b8615c3fc14c6" class="toggle" checked/><label for="section-e57a324c0a87423ed05b8615c3fc14c6" ></label><a href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/search/informed.html">Informierte Suche<i class="fas fa-check read-icon"></i></a><ul>
          <li data-nav-id="/search/informed/branchandbound.html" title="Suche mit Branch-and-Bound" class="dd-item"><a href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/search/informed/branchandbound.html">Branch-and-Bound<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/search/informed/bestfirst.html" title="Suche mit Best First" class="dd-item"><a href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/search/informed/bestfirst.html">Best First<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/search/informed/astar.html" title="Suche mit A*" class="dd-item"><a href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/search/informed/astar.html">A*<i class="fas fa-check read-icon"></i></a></li></ul></li>
          <li data-nav-id="/search/local.html" title="Lokale Suche" class="dd-item alwaysopen"><input type="checkbox" id="section-3df58c316d8cceefc4f029351d45366f" class="toggle" checked/><label for="section-3df58c316d8cceefc4f029351d45366f" ></label><a href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/search/local.html">Lokale Suche<i class="fas fa-check read-icon"></i></a><ul>
          <li data-nav-id="/search/local/gradient.html" title="Lokale Suche: Gradientensuche" class="dd-item"><a href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/search/local/gradient.html">Gradientensuche<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/search/local/annealing.html" title="Lokale Suche: Simulated Annealing" class="dd-item"><a href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/search/local/annealing.html">Simulated Annealing<i class="fas fa-check read-icon"></i></a></li></ul></li></ul></li>
          <li data-nav-id="/ea.html" title="Genetische Algorithmen" class="dd-item"><input type="checkbox" id="section-4427743bb3613ebd9fb288dea49d1f12" class="toggle"/><label for="section-4427743bb3613ebd9fb288dea49d1f12" ></label><a href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/ea.html">Genetische Algorithmen<i class="fas fa-check read-icon"></i></a><ul>
          <li data-nav-id="/ea/intro.html" title="Einführung Evolutionäre Algorithmen" class="dd-item"><a href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/ea/intro.html">Intro EA/GA<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/ea/ga.html" title="Modellierung mit Genetischen Algorithmen" class="dd-item"><a href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/ea/ga.html">Genetische Algorithmen<i class="fas fa-check read-icon"></i></a></li></ul></li>
          <li data-nav-id="/games.html" title="Spiele" class="dd-item"><input type="checkbox" id="section-dcab5a097c2e0bc19131dd59a2fac20e" class="toggle"/><label for="section-dcab5a097c2e0bc19131dd59a2fac20e" ></label><a href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/games.html">Spiele<i class="fas fa-check read-icon"></i></a><ul>
          <li data-nav-id="/games/intro.html" title="Einführung Optimale Spiele" class="dd-item"><a href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/games/intro.html">Optimale Spiele<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/games/minimax.html" title="Minimax" class="dd-item"><a href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/games/minimax.html">Minimax<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/games/heuristics.html" title="Heuristiken" class="dd-item"><a href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/games/heuristics.html">Heuristiken<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/games/alphabeta.html" title="Alpha-Beta-Pruning" class="dd-item"><a href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/games/alphabeta.html">Alpha-Beta-Pruning<i class="fas fa-check read-icon"></i></a></li></ul></li>
          <li data-nav-id="/csp.html" title="Constraintsolving" class="dd-item"><input type="checkbox" id="section-af49da79e06eb6041df4c439c33a1423" class="toggle"/><label for="section-af49da79e06eb6041df4c439c33a1423" ></label><a href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp.html">CSP<i class="fas fa-check read-icon"></i></a><ul>
          <li data-nav-id="/csp/intro.html" title="Einführung Constraints" class="dd-item"><a href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp/intro.html">Intro<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/csp/backtrackingsearch.html" title="Lösen von diskreten CSP" class="dd-item"><a href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp/backtrackingsearch.html">Lösen von diskreten CSP<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/csp/heuristics.html" title="Heuristiken" class="dd-item"><a href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp/heuristics.html">Heuristiken<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/csp/ac3.html" title="Kantenkonsistenz und AC-3" class="dd-item"><a href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/csp/ac3.html">AC-3<i class="fas fa-check read-icon"></i></a></li></ul></li>
          <li data-nav-id="/dtl.html" title="Entscheidungsbäume" class="dd-item"><input type="checkbox" id="section-4574116219561e15eeac8b4524c3245f" class="toggle"/><label for="section-4574116219561e15eeac8b4524c3245f" ></label><a href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl.html">Entscheidungsbäume<i class="fas fa-check read-icon"></i></a><ul>
          <li data-nav-id="/dtl/mlbasics.html" title="Machine Learning 101" class="dd-item"><a href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/mlbasics.html">Machine Learning 101<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/dtl/cal2.html" title="CAL2" class="dd-item"><a href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/cal2.html">CAL2<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/dtl/pruning.html" title="Pruning" class="dd-item"><a href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/pruning.html">Pruning<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/dtl/cal3.html" title="CAL3" class="dd-item"><a href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/cal3.html">CAL3<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/dtl/entropy.html" title="Entropie" class="dd-item"><a href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/entropy.html">Entropie<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/dtl/id3.html" title="ID3 und C4.5" class="dd-item"><a href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/dtl/id3.html">ID3 und C4.5<i class="fas fa-check read-icon"></i></a></li></ul></li>
          <li data-nav-id="/naivebayes.html" title="Naive Bayes" class="dd-item parent"><input type="checkbox" id="section-2de9e17830839ea2a34b596e2636549b" class="toggle" checked/><label for="section-2de9e17830839ea2a34b596e2636549b" ></label><a href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/naivebayes.html">Naive Bayes<i class="fas fa-check read-icon"></i></a><ul>
          <li data-nav-id="/naivebayes/probability.html" title="Wiederholung Wahrscheinlichkeitstheorie" class="dd-item active"><a href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/naivebayes/probability.html">Wahrscheinlichkeiten<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/naivebayes/nb.html" title="Klassifikation mit Naive Bayes" class="dd-item"><a href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/naivebayes/nb.html">Naive Bayes<i class="fas fa-check read-icon"></i></a></li></ul></li>
          <li data-nav-id="/nn.html" title="NN: Einführung in Neuronale Netze" class="dd-item"><input type="checkbox" id="section-59dd9ac0a3ddcf6aaf56356bae885f3c" class="toggle"/><label for="section-59dd9ac0a3ddcf6aaf56356bae885f3c" ></label><a href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn.html">Neuronale Netze<i class="fas fa-check read-icon"></i></a><ul>
          <li data-nav-id="/nn/nn1_perceptron.html" title="NN1 - Das Perzeptron" class="dd-item"><a href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn1_perceptron.html">NN1 - Perzeptron<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/nn/nn2_linear_regression.html" title="NN2 - Lineare Regression" class="dd-item"><a href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn2_linear_regression.html">NN2 - Lineare Regression<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/nn/nn3_logistic_regression.html" title="NN3 - Logistische Regression" class="dd-item"><a href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn3_logistic_regression.html">NN3 - Logistische Regression<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/nn/nn4_overfitting.html" title="NN4 - Overfitting und Regularisierung" class="dd-item"><a href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn4_overfitting.html">NN4 - Overfitting<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/nn/nn5_mlp.html" title="NN5 - Multilayer Perzeptron" class="dd-item"><a href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn5_mlp.html">NN5 - MLP<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/nn/nn6_backprop.html" title="NN6 - Backpropagation" class="dd-item"><a href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn6_backprop.html">NN6 - Backpropagation<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/nn/nn7_training_testing.html" title="NN7 - Training &amp; Testing" class="dd-item"><a href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn7_training_testing.html">NN7 - Training &amp; Testing<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/nn/nn8_testing.html" title="NN8 - Performanzanalyse" class="dd-item"><a href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/nn/nn8_testing.html">NN8 - Performanzanalyse<i class="fas fa-check read-icon"></i></a></li></ul></li>
        </ul>
        <div id="shortcuts">
          <div class="nav-title">More</div>
          <ul>
            <li><a class="padding" href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/index.html#vorlesung-2-sws"><i class='fas fa-bookmark'></i> Zeiten</a></li>
            <li><a class="padding" href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/index.html#fahrplan"><i class='fas fa-bookmark'></i> Fahrplan</a></li>
            <li><a class="padding" href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/assignments.html"><i class='fas fa-bookmark'></i> Praktikum/Übung</a></li>
            <li><a class="padding" href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/index.html#pr%C3%BCfungsform-note-und-credits"><i class='fas fa-bookmark'></i> Note/Credits</a></li>
            <li><a class="padding" href="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/org/faq.html"><i class='fas fa-bookmark'></i> FAQ</a></li>
          </ul>
        </div>
        <div class="footermargin footerLangSwitch footerVariantSwitch footerVisitedLinks footerFooter showVariantSwitch showVisitedLinks showFooter"></div>
        <hr class="default-animation footerLangSwitch footerVariantSwitch footerVisitedLinks footerFooter showVariantSwitch showVisitedLinks showFooter"/>
        <div id="prefooter" class="footerLangSwitch footerVariantSwitch footerVisitedLinks showVariantSwitch showVisitedLinks">
          <ul>
            <li id="select-language-container" class="footerLangSwitch">
              <a class="padding select-container">
                <i class="fas fa-language fa-fw"></i>
                <span>&nbsp;</span>
                <div class="select-style">
                  <select id="select-language" onchange="location = baseUri + this.value;">
                  </select>
                </div>
                <div class="select-clear"></div>
              </a>
            </li>
            <li id="select-variant-container" class="footerVariantSwitch showVariantSwitch">
              <a class="padding select-container">
                <i class="fas fa-paint-brush fa-fw"></i>
                <span>&nbsp;</span>
                <div class="select-style">
                  <select id="select-variant" onchange="window.variants && variants.changeVariant( this.value );">
                    <option id="relearn-light" value="relearn-light" selected>Relearn Light</option>
                    <option id="relearn-dark" value="relearn-dark">Relearn Dark</option>
                    <option id="green" value="green">Green</option>
                  </select>
                </div>
                <div class="select-clear"></div>
              </a>
              <script>window.variants && variants.markSelectedVariant();</script>
            </li>
            <li class="footerVisitedLinks showVisitedLinks"><a class="padding" onclick="clearHistory();"><i class="fas fa-history fa-fw"></i> Clear History</a></li>
          </ul>
        </div>
        <div id="footer" class="footerFooter showFooter"><p style="text-align: center; padding: 2rem 1rem;">
<i class='fab fa-fw fa-github'></i><a href="https://github.com/Artificial-Intelligence-FHB-TDU/KI-Vorlesung">GitHub</a>
<i class='fas fa-fw fa-tags'></i><a href="https://github.com/Artificial-Intelligence-FHB-TDU/KI-Vorlesung/issues">Issues</a>
<i class='fas fa-fw fa-bullhorn'></i><a href="https://github.com/Artificial-Intelligence-FHB-TDU/KI-Vorlesung/blob/master/CREDITS.md">Credits</a>

<br />
<a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0;margin:0;display:inline;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
</p>

        </div>
      </div>
    </aside>
    <script src="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/js/clipboard.min.js?1667412025" defer></script>
    <script src="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/js/perfect-scrollbar.min.js?1667412025" defer></script>
    <script src="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/js/featherlight.min.js?1667412025" defer></script>
    <script>
      function useMathJax( config ){
        if( !Object.assign ){
          
          return;
        }
        window.MathJax = Object.assign( window.MathJax || {}, {
          loader: {
            load: ['[tex]/mhchem']
          },
          startup: {
            elements: [
              '.math'
            ]
          },
          tex: {
            inlineMath: [
              ['$', '$'], 
              ['\\(', '\\)']
            ]
          },
          options: {
            enableMenu: false 
          }
        }, config );
      }
      useMathJax( JSON.parse("{}") );
    </script>
    <script id="MathJax-script" async src="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/js/mathjax/tex-mml-chtml.js?1667412025"></script>
    <script src="https://www.fh-bielefeld.de/elearning/data/FH-Bielefeld/lm_data/lm_1358898/js/theme.js?1667412025" defer></script>
  </body>
</html>
